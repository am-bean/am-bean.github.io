@misc{beanLargeLanguageModels2023,
  title = {Large Language Models - {{Written}} Evidence},
  author = {Bean, Andrew M. and Kirk, Hannah Rose and M{\"o}kander, Jakob and Osborne, Cailean and Roberts, Huw and Ziosi, Marta},
  year = {2023},
  month = oct,
  number = {LLM0074},
  publisher = {{UK Parliament}},
  langid = {english},
  show_url = {true},
  url = {https://committees.parliament.uk/writtenevidence/124308/pdf/},
  selected={false},
}

@misc{khandelwalCasteistNotRacist2023,
  title = {Casteist but {{Not Racist}}? {{Quantifying Disparities}} in {{Large Language Model Bias}} between {{India}} and the {{West}}},
  shorttitle = {Casteist but {{Not Racist}}?},
  author = {Khandelwal, Khyati and Tonneau, Manuel and Bean, Andrew M. and Kirk, Hannah Rose and Hale, Scott A.},
  year = {2023},
  month = sep,
  arxiv = {2309.08573},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.08573},
  abstract = {Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. A large body of scholarship on LLM bias exists but it predominantly adopts a Western-centric frame and attends comparatively less to bias levels and potential harms in the Global South. In this paper, we quantify stereotypical bias in popular LLMs according to an Indian-centric frame and compare bias levels between the Indian and Western contexts. To do this, we develop a novel dataset which we call Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context. We finally investigate Instruction Prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for GPT-3.5. The findings of this work highlight the need for including more diverse voices when evaluating LLMs.},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  selected={false}
}

@misc{kirkPresentBetterFuture2023,
  title = {The {{Past}}, {{Present}} and {{Better Future}} of {{Feedback Learning}} in {{Large Language Models}} for {{Subjective Human Preferences}} and {{Values}}},
  author = {Kirk, Hannah Rose and Bean, Andrew M. and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A.},
  year = {2023},
  month = oct,
  arxiv = {2310.07629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07629},
  abstract = {Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  selected={true},
}

@misc{beanExploringLandscapeLarge2024,
  title = {Exploring the Landscape of Large Language Models in Medical Question Answering},
  author = {Bean, Andrew M. and Korgul, Karolina and Krones, Felix and McCraith, Robert and Mahdi, Adam},
  year = {2024},
  month = mar,
  arxiv = {2310.07225},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07225},
  urldate = {2024-04-24},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  abstract = {With the rapid development of new large language models (LLMs), each claiming to surpass previous models, an overall picture of medical LLM research can be elusive. To address this challenge, we benchmark a range of top LLMs and identify consistent patterns which appear across models. We test 8 well-known LLMs on 874 newly collected questions from Polish medical licensing exams. For each question, we score each model on the top-1 accuracy and the distribution of probabilities assigned. We then compare with factors including question difficulty for humans, question length, and the scores of the other models. LLM accuracies were positively correlated pairwise (0.29 to 0.62). Model performance was also correlated with human performance (0.07 to 0.16), but negatively correlated to the difference between the question-level accuracy of top-scoring and bottom-scoring humans (−0.16 to −0.23). The top output probability and question length were positive and negative predictors of accuracy respectively (p <0.05). The top scoring LLM, GPT-4 Turbo, scored 82%, followed by Med42, PaLM 2, Mixtral and GPT-3.5 around 63%. We found evidence of similarities between models in which questions they answer correctly, as well as similarities with human test takers. Larger models typically performed better, but differences in training methods were also highly impactful. Model accuracy was positively correlated with confidence, but negatively correlated with question length. We expect that similar training methods will lead these patterns to persist across future models. These patterns can therefore aid medical experts in forming expectations about LLMs as a category to support application research.},
  selected={false},

}

@misc{kirkPRISMAlignmentProject2024,
  title = {The {{PRISM Alignment Project}}: {{What Participatory}}, {{Representative}} and {{Individualised Human Feedback Reveals About}} the {{Subjective}} and {{Multicultural Alignment}} of {{Large Language Models}}},
  shorttitle = {The {{PRISM Alignment Project}}},
  author = {Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew M. and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
  year = {2024},
  month = apr,
  number = {arXiv:2404.16019},
  arxiv = {2404.16019},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.16019},
  urldate = {2024-04-25},
  abstract = {Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  selected={true}
}


@misc{beanLINGOLYBenchmarkOlympiadLevel2024,
  title = {{{LINGOLY}}: {{A Benchmark}} of {{Olympiad-Level Linguistic Reasoning Puzzles}} in {{Low-Resource}} and {{Extinct Languages}}},
  shorttitle = {{{LINGOLY}}},
  author = {Bean, Andrew M. and Hellsten, Simi and Mayne, Harry and Magomere, Jabez and Chi, Ethan A. and Chi, Ryan and Hale, Scott A. and Kirk, Hannah Rose},
  year = {2024},
  month = jun,
  arxiv={2406.06196},
  selected={True}
}
